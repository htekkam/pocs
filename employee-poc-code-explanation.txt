Code to upload parquet file to S3 by reading a record from employee table 
=======================================================================
import json import boto3 import pandas as pd import io from pymongo import 
MongoClient s3 = boto3.client('s3') 

def lambda_handler(event, context): 
# 1️⃣ Connect to MongoDB Atlas uri = "mongodb+srv://mongodb:mongodb@cluster0.oadmus1.mongodb.net/" client = MongoClient(uri) db = client["java-mongo-integration"] collection = db["employee"] 
# 2️⃣ Fetch records employees = list(collection.find({}, {"_id": 0})) 
# 3️⃣ Convert to Parquet df = pd.DataFrame(employees) buffer = io.BytesIO() df.to_parquet(buffer, index=False) 
# 4️⃣ Upload to S3 s3.put_object( Bucket="employee-data-bucket-20251027", Key="employee_data/employees.parquet", Body=buffer.getvalue() ) return { 'statusCode': 200, 'body': json.dumps("Parquet file uploaded successfully!") }

Imports:
==========================
import json
import boto3
import pandas as pd
import io
from pymongo import MongoClient

What each import does?
=================================

| Library        | Purpose                                                                                                  |
| -------------- | -------------------------------------------------------------------------------------------------------- |
| `json`         | Used to format the output (especially in Lambda responses).                                              |
| `boto3`        | AWS SDK for Python — lets you interact with AWS services like S3, Lambda, etc.                           |
| `pandas as pd` | Used for data manipulation; here, it converts MongoDB data into a DataFrame and then exports to Parquet. |
| `io`           | Provides in-memory file-like objects (`BytesIO`) for writing data without saving it locally.             |
| `pymongo`      | MongoDB client library — lets you connect and query MongoDB databases.                                   |

S3 client creation:
==============================
s3 = boto3.client('s3')

=> This initializes an S3 client using boto3.

=> Lambda automatically gets AWS credentials via its execution role, so no need to pass access keys manually.

=> You’ll use this client to upload the Parquet file later.

Lambda Handler:
=======================
def lambda_handler(event, context):

-> This is the entry point for the AWS Lambda function.

-> AWS automatically calls this method when your Lambda is triggered (manually, via S3 event, API Gateway, etc.).

Parameters:

->event → carries input data to the function.

->context → provides runtime information like memory, time remaining, and request ID.

1. Connecting to MongoDB Atlas:
---------------------------------
-> uri = "mongodb+srv://mongodb:mongodb@cluster0.oadmus1.mongodb.net/"
-> client = MongoClient(uri)
-> db = client["java-mongo-integration"]
-> collection = db["employee"]

Explanation:
================================================================
-> uri: MongoDB connection string — includes credentials and cluster info.

In this case:

-> username = mongodb

-> password = mongodb

-> cluster = cluster0.oadmus1.mongodb.net

-> MongoClient(uri) → establishes connection with MongoDB Atlas.

-> db → selects the database named "java-mongo-integration".

-> collection → selects the "employee" collection inside that database.


2. Fetch Records:
=====================
        employees = list(collection.find({}, {"_id": 0}))
        
        Explanation:
        
        find({}, {"_id": 0})
        
        {} → empty filter means “fetch all records”.
        
        {"_id": 0} → projection that excludes the MongoDB internal _id field.
        
        Converts the cursor object into a list of dictionaries, e.g.:
        
        [
          {"empId": 101, "name": "Hari", "salary": 60000},
          {"empId": 102, "name": "Ravi", "salary": 55000}
        ]


3. Convert to Parquet Format:
=======================================
        df = pd.DataFrame(employees)
        buffer = io.BytesIO()
        df.to_parquet(buffer, index=False)

                Explanation:
                ============
                
                -> pd.DataFrame(employees) → converts list of dicts into a Pandas DataFrame.
                
                -> buffer = io.BytesIO() → creates an in-memory binary buffer (acts like a temporary file in RAM).
                
                -> df.to_parquet(buffer, index=False) → writes the DataFrame in Parquet format (a compressed, columnar storage format) into that buffer.
                
                ✅ This avoids saving files locally — which is ideal for AWS Lambda (ephemeral storage and performance).


4️⃣ Upload to S3:
======================
s3.put_object(
    Bucket="employee-data-bucket-20251027",
    Key="employee_data/employees.parquet",
    Body=buffer.getvalue()
)

        Explanation:
        ============================
        
        -> Bucket → your target S3 bucket name.
        
        -> Key → path inside the bucket (folder/fileName).
        
       ->  Body → actual binary content of the Parquet file (from the buffer).

        Result:
        File is stored in S3 at
        s3://employee-data-bucket-20251027/employee_data/employees.parquet


